{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_storage import create_connection\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = create_connection(\"../database/crypto_billionairs.db\")\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train):\n",
    "    \n",
    "    clf = RandomForestClassifier(criterion=\"entropy\", min_samples_split= 0.01, min_samples_leaf= 0.005, max_depth=10, class_weight=\"balanced_subsample\")\n",
    "    print(\"training random forest!\")\n",
    "    clf.fit(X_train, y_train)\n",
    "   \n",
    "    return \"random_forest\", clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train):\n",
    "    \n",
    "    neigh = KNeighborsClassifier(weights=\"uniform\", n_neighbors=5, algorithm=\"ball_tree\")\n",
    "    print(\"training knn!\")\n",
    "    neigh.fit(X_train, y_train)\n",
    "    \n",
    "    return \"knn\", neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine(X_train, y_train):\n",
    "    \n",
    "    svc = SVC(kernel=\"poly\", degree=4, C=1)\n",
    "    print(\"training svc!\")\n",
    "    svc.fit(X_train, y_train)\n",
    "    \n",
    "    return \"support_vector_classifier\", svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train, y_train):\n",
    "    \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"tanh\", solver=\"lbfgs\", learning_rate=\"constant\", learning_rate_init=2e-5, tol=1e-5)\n",
    "    print(\"training mlp!\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    return \"mlp_classifier\", mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train):\n",
    "    global lg\n",
    "    lg = LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=1)\n",
    "    print(\"training lr!\")\n",
    "    lg.fit(X_train, y_train)\n",
    "    \n",
    "    return \"logistic_regression\", lg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ensemble(X_train, y_train):\n",
    "    \n",
    "    print(\"training ensemble!\")\n",
    "    global rf_new\n",
    "    \n",
    "    level0 = list()\n",
    "    level0.append(('lg', LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=1)))\n",
    "    level0.append(('knn', KNeighborsClassifier(weights=\"uniform\", n_neighbors=5, algorithm=\"ball_tree\")))\n",
    "    level0.append(('rf', RandomForestClassifier(criterion=\"entropy\", min_samples_split= 0.01, min_samples_leaf= 0.005, max_depth=10, class_weight=\"balanced_subsample\")))\n",
    "    level0.append(('mlp', MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"tanh\", solver=\"lbfgs\", learning_rate=\"constant\", learning_rate_init=2e-5, tol=1e-5)))\n",
    "    #mlp = MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"tanh\", solver=\"lbfgs\", learning_rate=\"constant\", learning_rate_init=2e-5, tol=1e-5)\n",
    "    #mlp.fit(ensemble_dataset, y_train)\n",
    "    \n",
    "    level1 = RandomForestClassifier(criterion=\"entropy\")\n",
    "    rf_new = StackingClassifier(estimators=level0, final_estimator=level1)\n",
    "    rf_new.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"finished training ensemble!\")\n",
    "    return \"ensemble\", rf_new\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(X_test, y_test, model, model_name, table, db_connection):\n",
    "    \n",
    "    X_test_normalized = X_test.copy()\n",
    "    X_test_normalized[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_test_normalized[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "    y_pred = model.predict(X_test_normalized)\n",
    "    \n",
    "    f1score = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1score_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    y_pred = y_pred.tolist()\n",
    "    df = pd.concat([X_test])\n",
    "    df[\"buy_short_indicator\"] = y_pred\n",
    "    df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "    \n",
    "    df.to_sql(f\"{table}_{model_name}_pooling\", db_connection, if_exists=\"replace\")\n",
    "    \n",
    "    return f\"{model_name}_pooling\", f1score, f1score_macro, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_ensemble(X_test, y_test, model, model_name, table, db_connection):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        f1score = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        f1score_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "        recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "        precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "        y_pred = y_pred.tolist()\n",
    "        df = pd.concat([X_test])\n",
    "        df[\"buy_short_indicator\"] = y_pred\n",
    "        df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "    \n",
    "        df.to_sql(f\"{table}_{model_name}_pooling\", db_connection, if_exists=\"replace\")\n",
    "    \n",
    "        return f\"{model_name}_pooling\", f1score, f1score_macro, recall, precision\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ml_algorithms_pooling(db_connection):\n",
    "    \n",
    "    df_temp = pd.read_sql_query(f\"select * from cryptocurrency_pooling_dataset\", db_connection)\n",
    "    df_temp = shuffle(df_temp, random_state=42069)\n",
    "    \n",
    "    y_train = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "    y_train = y_train.fillna(0)\n",
    "    y_train = y_train.astype(str)\n",
    "        \n",
    "    X_train = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\",\"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1)\n",
    "    \n",
    "    X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\", db_connection)\n",
    "    table_names_list = table_names['name'].tolist()\n",
    "    filtered_table_names = [name for name in table_names_list if \"_1day_features\" in name and 'trades' not in name and 'equity_curve' not in name and '_pooling' not in name and \"_threshold_ensemble\" not in name]\n",
    "    print(filtered_table_names)\n",
    "    \n",
    "    rf_name, rf_model = random_forest(X_resampled, y_resampled)\n",
    "    knn_name, knn_model = knn(X_resampled, y_resampled)\n",
    "    svc_name, svc_model = support_vector_machine(X_resampled, y_resampled)\n",
    "    mlp_name, mlp_model = mlp(X_resampled, y_resampled)\n",
    "    lr_name, lr_model = logistic_regression(X_resampled, y_resampled)\n",
    "    \n",
    "    ensemble_name, ensemble_model = model_ensemble(X_resampled, y_resampled)\n",
    "    #creating the evaluation metric\n",
    "    df_ml = pd.DataFrame(columns = range(6))\n",
    "    df_ml.columns = [\"table_name\", \"model\", \"f1-score weighted\", \"f1-score macro\", \"recall macro\", \"precision macro\"]\n",
    "    \n",
    "    \n",
    "    for table in filtered_table_names:\n",
    "        \n",
    "        df_temp = pd.read_sql_query(f\"select * from {table}\", db_connection)\n",
    "        y = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "        y = y.fillna(0)\n",
    "        y = y.astype(str)\n",
    "        X = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\",\"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1)\n",
    "\n",
    "        X_test = X.iloc[-365:].interpolate()\n",
    "        y_test = y.iloc[-365:]\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision= evaluation(X_test, y_test, rf_model, rf_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision= evaluation(X_test, y_test, knn_model, knn_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision = evaluation(X_test, y_test, mlp_model, mlp_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision = evaluation(X_test, y_test, svc_model, svc_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision = evaluation(X_test, y_test, lr_model, lr_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "\n",
    "        string, score, f1score_macro, recall, precision = evaluation_ensemble(X_test, y_test, ensemble_model, ensemble_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "    return df_ml\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADA_1min_complete_1day_preprocessed_1day_features', 'BCH_1min_complete_1day_preprocessed_1day_features', 'BSV_1min_complete_1day_preprocessed_1day_features', 'BTC_1min_complete_1day_preprocessed_1day_features', 'BTG_1min_complete_1day_preprocessed_1day_features', 'DASH_1min_complete_1day_preprocessed_1day_features', 'DOGE_1min_complete_1day_preprocessed_1day_features', 'EOS_1min_complete_1day_preprocessed_1day_features', 'ETC_1min_complete_1day_preprocessed_1day_features', 'ETH_1min_complete_1day_preprocessed_1day_features', 'FUN_1min_complete_1day_preprocessed_1day_features', 'ICX_1min_complete_1day_preprocessed_1day_features', 'KNC_1min_complete_1day_preprocessed_1day_features', 'LINK_1min_complete_1day_preprocessed_1day_features', 'LRC_1min_complete_1day_preprocessed_1day_features', 'LTC_1min_complete_1day_preprocessed_1day_features', 'MKR_1min_complete_1day_preprocessed_1day_features', 'NEO_1min_complete_1day_preprocessed_1day_features', 'OMG_1min_complete_1day_preprocessed_1day_features', 'ONT_1min_complete_1day_preprocessed_1day_features', 'QTUM_1min_complete_1day_preprocessed_1day_features', 'REP_1min_complete_1day_preprocessed_1day_features', 'SNT_1min_complete_1day_preprocessed_1day_features', 'TRX_1min_complete_1day_preprocessed_1day_features']\n",
      "training random forest!\n",
      "training knn!\n",
      "training svc!\n",
      "training mlp!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lr!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ensemble!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished training ensemble!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:467: RuntimeWarning: invalid value encountered in true_divide\n",
      "  prob /= prob.sum(axis=1).reshape((prob.shape[0], -1))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-f814fe7a44ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#%%capture\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_ml_algorithms_pooling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-f25d1149bd5f>\u001b[0m in \u001b[0;36mapply_ml_algorithms_pooling\u001b[1;34m(db_connection)\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mdf_ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1score_macro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1score_macro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluation_ensemble\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensemble_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensemble_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m         \u001b[0mdf_ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1score_macro\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "df_ml = apply_ml_algorithms_pooling(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>model</th>\n",
       "      <th>f1-score weighted</th>\n",
       "      <th>f1-score macro</th>\n",
       "      <th>recall macro</th>\n",
       "      <th>precision macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>random_forest_pooling</td>\n",
       "      <td>0.611848</td>\n",
       "      <td>0.412580</td>\n",
       "      <td>0.424004</td>\n",
       "      <td>0.411183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>knn_pooling</td>\n",
       "      <td>0.077527</td>\n",
       "      <td>0.099956</td>\n",
       "      <td>0.344697</td>\n",
       "      <td>0.316983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>mlp_classifier_pooling</td>\n",
       "      <td>0.029379</td>\n",
       "      <td>0.076052</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.042922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>logistic_regression_pooling</td>\n",
       "      <td>0.526882</td>\n",
       "      <td>0.315462</td>\n",
       "      <td>0.382155</td>\n",
       "      <td>0.320606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.333500</td>\n",
       "      <td>0.222414</td>\n",
       "      <td>0.372555</td>\n",
       "      <td>0.300068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>random_forest_pooling</td>\n",
       "      <td>0.616998</td>\n",
       "      <td>0.395713</td>\n",
       "      <td>0.433490</td>\n",
       "      <td>0.401590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>knn_pooling</td>\n",
       "      <td>0.046267</td>\n",
       "      <td>0.096394</td>\n",
       "      <td>0.314266</td>\n",
       "      <td>0.212135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>mlp_classifier_pooling</td>\n",
       "      <td>0.678087</td>\n",
       "      <td>0.349945</td>\n",
       "      <td>0.368377</td>\n",
       "      <td>0.333381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>logistic_regression_pooling</td>\n",
       "      <td>0.559864</td>\n",
       "      <td>0.294621</td>\n",
       "      <td>0.356898</td>\n",
       "      <td>0.308217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.201322</td>\n",
       "      <td>0.143532</td>\n",
       "      <td>0.330315</td>\n",
       "      <td>0.301984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            table_name  \\\n",
       "0    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "1    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "2    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "3    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "4    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "..                                                 ...   \n",
       "115  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "116  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "117  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "118  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "119  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "\n",
       "                           model  f1-score weighted  f1-score macro  \\\n",
       "0          random_forest_pooling           0.611848        0.412580   \n",
       "1                    knn_pooling           0.077527        0.099956   \n",
       "2         mlp_classifier_pooling           0.029379        0.076052   \n",
       "3    logistic_regression_pooling           0.526882        0.315462   \n",
       "4               ensemble_pooling           0.333500        0.222414   \n",
       "..                           ...                ...             ...   \n",
       "115        random_forest_pooling           0.616998        0.395713   \n",
       "116                  knn_pooling           0.046267        0.096394   \n",
       "117       mlp_classifier_pooling           0.678087        0.349945   \n",
       "118  logistic_regression_pooling           0.559864        0.294621   \n",
       "119             ensemble_pooling           0.201322        0.143532   \n",
       "\n",
       "     recall macro  precision macro  \n",
       "0        0.424004         0.411183  \n",
       "1        0.344697         0.316983  \n",
       "2        0.333333         0.042922  \n",
       "3        0.382155         0.320606  \n",
       "4        0.372555         0.300068  \n",
       "..            ...              ...  \n",
       "115      0.433490         0.401590  \n",
       "116      0.314266         0.212135  \n",
       "117      0.368377         0.333381  \n",
       "118      0.356898         0.308217  \n",
       "119      0.330315         0.301984  \n",
       "\n",
       "[120 rows x 6 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_argmax_evaluation(model, X_train, table, db_connection):\n",
    "    \n",
    "    class_probabilities = model.predict_proba(X_train)\n",
    "    thresholds_long = []\n",
    "    for i in np.arange(0, 1, 0.02):\n",
    "        thresholds_long.append(i)\n",
    "        \n",
    "    i = 0\n",
    "    for threshold in thresholds_long:\n",
    "        class1 = class_probabilities[:, 0].copy()\n",
    "\n",
    "        class1[class1 > threshold] = 1\n",
    "        class1[class1 < threshold] = 0\n",
    "        \n",
    "        class1_str = [str(x) for x in class1.tolist()]\n",
    "        \n",
    "        df = pd.concat([X_train])\n",
    "        df[\"buy_short_indicator\"] = class1_str\n",
    "        df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "        df.to_sql(f\"no_{i}_threshold_ensemble_long_{table[:5]}\", db_connection, if_exists=\"replace\")\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    thresholds_short = []\n",
    "    for i in np.arange(0, 1, 0.02):\n",
    "        thresholds_short.append(i)\n",
    "    \n",
    "    k = 0\n",
    "    for threshold_short in thresholds_short:\n",
    "        print(k)\n",
    "        class3 = class_probabilities[:, 2].copy()\n",
    "\n",
    "        class3[class3 < threshold_short] = 0\n",
    "        class3[class3 > threshold_short] = -1\n",
    "        class3_str = [str(x) for x in class3.tolist()]\n",
    "        \n",
    "        df = pd.concat([X_train])\n",
    "        df[\"buy_short_indicator\"] = class3_str\n",
    "        df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "        df.to_sql(f\"no_{k}_threshold_ensemble_short_{table[:5]}\", db_connection, if_exists=\"replace\")\n",
    "        \n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_alternative_argmax_evaluation(db_connection):\n",
    "\n",
    "    table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\", db_connection)\n",
    "    \n",
    "    table_names_list = table_names['name'].tolist()\n",
    "\n",
    "    filtered_table_names = [name for name in table_names_list if \"_1day_features\" in name and 'trades' not in name and 'equity_curve' not in name and '_pooling' not in name and \"ensemble\" not in name]\n",
    "    print(filtered_table_names)\n",
    "    for table in filtered_table_names:\n",
    "        df_temp = pd.read_sql_query(f\"select * from {table}\", connection)\n",
    "        \n",
    "        y = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "        y = y.fillna(0)\n",
    "        y = y.astype(str)\n",
    "        \n",
    "        X = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\", \"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1) \n",
    "        X_train = X.iloc[:-365]\n",
    "\n",
    "        X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "\n",
    "        \n",
    "        alternative_argmax_evaluation(rf_new, X_train, table, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_alternative_argmax_evaluation(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_argmax_evaluation(model, X_test, y_test, table, db_connection):\n",
    "    \n",
    "    class_probabilities = model.predict_proba(X_test)\n",
    "    \n",
    "    class1 = class_probabilities[:, 0].copy()\n",
    "    class3 = class_probabilities[:, 2].copy()\n",
    "    \n",
    "    class1[class1 >= 0.66] = 1\n",
    "    class1[class1 < 0.66] = 0\n",
    "\n",
    "    class3[class3 < 0.78] = 0\n",
    "    class3[class3 >= 0.78] = -1\n",
    "\n",
    "    \n",
    "    y_pred = class3 + class1\n",
    "    y_pred1 = y_pred.tolist()\n",
    "    y_pred = [str(x) for x in y_pred1]\n",
    "    print(y_pred)\n",
    "    f1score = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1score_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    df = pd.concat([X_test])\n",
    "    df[\"buy_short_indicator\"] = y_pred\n",
    "    df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "    \n",
    "    df.to_sql(f\"ensemble_pooling_final_{table[:5]}\", db_connection, if_exists=\"replace\")\n",
    "    \n",
    "    return f1score, f1score_macro, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_alternative_argmax_evaluation(db_connection):\n",
    "\n",
    "    table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\", db_connection)\n",
    "    \n",
    "    table_names_list = table_names['name'].tolist()\n",
    "\n",
    "    filtered_table_names = [name for name in table_names_list if \"_1day_features\" in name and 'trades' not in name and 'equity_curve' not in name and '_pooling' not in name and \"ensemble\" not in name]\n",
    "    print(filtered_table_names)\n",
    "    for table in filtered_table_names:\n",
    "        df_temp = pd.read_sql_query(f\"select * from {table}\", connection)\n",
    "        \n",
    "        y = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "        y = y.fillna(0)\n",
    "        y = y.astype(str)\n",
    "        \n",
    "        X = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\",\"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1)\n",
    "        \n",
    "        \n",
    "        X_test = X.iloc[-365:]\n",
    "        X_test[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_test[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "        y_test = y.iloc[-365:]\n",
    "\n",
    "        \n",
    "        f1score, f1score_macro, recall, precision = best_argmax_evaluation(rf_new, X_test, y_test, table, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_alternative_argmax_evaluation(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf7c0b8ed4c815043e48994c1e64c08f9d96fdc49c73ce762547f36d7ce0a11b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
