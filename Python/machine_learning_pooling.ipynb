{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_storage import create_connection\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = create_connection(\"../database/crypto_billionairs.db\")\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train):\n",
    "    \n",
    "    clf = RandomForestClassifier(criterion=\"entropy\", min_samples_split= 0.01, min_samples_leaf= 0.005, max_depth=10, class_weight=\"balanced_subsample\")\n",
    "    print(\"training random forest!\")\n",
    "    clf.fit(X_train, y_train)\n",
    "   \n",
    "    return \"random_forest\", clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train):\n",
    "    \n",
    "    neigh = KNeighborsClassifier(weights=\"uniform\", n_neighbors=5, algorithm=\"ball_tree\")\n",
    "    print(\"training knn!\")\n",
    "    neigh.fit(X_train, y_train)\n",
    "    \n",
    "    return \"knn\", neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine(X_train, y_train):\n",
    "    \n",
    "    svc = SVC(kernel=\"poly\", degree=4, C=1)\n",
    "    print(\"training svc!\")\n",
    "    svc.fit(X_train, y_train)\n",
    "    \n",
    "    return \"support_vector_classifier\", svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train, y_train):\n",
    "    \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"tanh\", solver=\"lbfgs\", learning_rate=\"constant\", learning_rate_init=2e-5, tol=1e-5)\n",
    "    print(\"training mlp!\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    return \"mlp_classifier\", mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train):\n",
    "    global lg\n",
    "    lg = LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=1)\n",
    "    print(\"training lr!\")\n",
    "    lg.fit(X_train, y_train)\n",
    "    \n",
    "    return \"logistic_regression\", lg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ensemble(X_train, y_train):\n",
    "    \n",
    "    print(\"training ensemble!\")\n",
    "    global rf_new\n",
    "    \n",
    "    level0 = list()\n",
    "    level0.append(('lg', LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=1)))\n",
    "    level0.append(('knn', KNeighborsClassifier(weights=\"uniform\", n_neighbors=5, algorithm=\"ball_tree\")))\n",
    "    level0.append(('rf', RandomForestClassifier(criterion=\"entropy\", min_samples_split= 0.01, min_samples_leaf= 0.005, max_depth=10, class_weight=\"balanced_subsample\")))\n",
    "    level0.append(('mlp', MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"tanh\", solver=\"lbfgs\", learning_rate=\"constant\", learning_rate_init=2e-5, tol=1e-5)))\n",
    "    #mlp = MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"tanh\", solver=\"lbfgs\", learning_rate=\"constant\", learning_rate_init=2e-5, tol=1e-5)\n",
    "    #mlp.fit(ensemble_dataset, y_train)\n",
    "    \n",
    "    level1 = RandomForestClassifier(criterion=\"entropy\")\n",
    "    rf_new = StackingClassifier(estimators=level0, final_estimator=level1)\n",
    "    rf_new.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"finished training ensemble!\")\n",
    "    return \"ensemble\", rf_new\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(X_test, y_test, model, model_name, table, db_connection):\n",
    "    \n",
    "    X_test_normalized = X_test.copy()\n",
    "    X_test_normalized[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_test_normalized[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "    y_pred = model.predict(X_test_normalized)\n",
    "    y_pred_proba = model.predict_proba(X_test_normalized)\n",
    "    \n",
    "    f1score = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1score_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    y_pred = y_pred.tolist()\n",
    "    df = pd.concat([X_test])\n",
    "    df[\"buy_short_indicator\"] = y_pred\n",
    "    df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "    \n",
    "    df.to_sql(f\"{table}_{model_name}_pooling\", db_connection, if_exists=\"replace\")\n",
    "    \n",
    "    return f\"{model_name}_pooling\", f1score, f1score_macro, recall, precision, y_pred_proba[:, 0], y_pred_proba[:, 1], y_pred_proba[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_ensemble(X_test, y_test, model, model_name, table, db_connection):\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    f1score = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1score_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    y_pred = y_pred.tolist()\n",
    "    df = pd.concat([X_test])\n",
    "    df[\"buy_short_indicator\"] = y_pred\n",
    "    df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "    \n",
    "    df.to_sql(f\"{table}_{model_name}_pooling\", db_connection, if_exists=\"replace\")\n",
    "    \n",
    "    return f\"{model_name}_pooling\", f1score, f1score_macro, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ml_algorithms_pooling(db_connection):\n",
    "    \n",
    "    df_temp = pd.read_sql_query(f\"select * from cryptocurrency_pooling_dataset\", db_connection)\n",
    "    df_temp = shuffle(df_temp, random_state=42069)\n",
    "    \n",
    "    y_train = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "    y_train = y_train.fillna(0)\n",
    "    y_train = y_train.astype(str)\n",
    "        \n",
    "    X_train = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\",\"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1)\n",
    "    \n",
    "    X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\", db_connection)\n",
    "    table_names_list = table_names['name'].tolist()\n",
    "    filtered_table_names = [name for name in table_names_list if \"_1day_features\" in name and 'trades' not in name and 'equity_curve' not in name and '_pooling' not in name and \"_threshold_ensemble\" not in name]\n",
    "    print(filtered_table_names)\n",
    "    \n",
    "    # rf_name, rf_model = random_forest(X_resampled, y_resampled)\n",
    "    # knn_name, knn_model = knn(X_resampled, y_resampled)\n",
    "    #svc_name, svc_model = support_vector_machine(X_resampled, y_resampled)\n",
    "    # mlp_name, mlp_model = mlp(X_resampled, y_resampled)\n",
    "    # lr_name, lr_model = logistic_regression(X_resampled, y_resampled)\n",
    "    \n",
    "    ensemble_name, ensemble_model = model_ensemble(X_resampled, y_resampled)\n",
    "    #creating the evaluation metric\n",
    "    df_ml = pd.DataFrame(columns = range(6))\n",
    "    df_ml.columns = [\"table_name\", \"model\", \"f1-score weighted\", \"f1-score macro\", \"recall macro\", \"precision macro\"]\n",
    "    \n",
    "    \n",
    "    for table in filtered_table_names:\n",
    "        \n",
    "        df_temp = pd.read_sql_query(f\"select * from {table}\", db_connection)\n",
    "        y = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "        y = y.fillna(0)\n",
    "        y = y.astype(str)\n",
    "        X = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\",\"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1)\n",
    "\n",
    "        X_test = X.iloc[-365:]\n",
    "        y_test = y.iloc[-365:]\n",
    "        \n",
    "        # string, score, f1score_macro, recall, precision, rf_pred_class1, rf_pred_class0, rf_pred_class_negative_1 = evaluation(X_test, y_test, rf_model, rf_name, table, connection)\n",
    "        # df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        # string, score, f1score_macro, recall, precision, knn_pred_class1, knn_pred_class0, knn_pred_class_negative_1 = evaluation(X_test, y_test, knn_model, knn_name, table, connection)\n",
    "        # df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        # string, score, f1score_macro, recall, precision = evaluation(X_test, y_test, svc_model, svc_name, table, connection)\n",
    "        # df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        # string, score, f1score_macro, recall, precision, mlp_pred_class1, mlp_pred_class0, mlp_pred_class_negative_1 = evaluation(X_test, y_test, mlp_model, mlp_name, table, connection)\n",
    "        # df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        # string, score, f1score_macro, recall, precision, lr_pred_class1, lr_pred_class0, lr_pred_class_negative_1 = evaluation(X_test, y_test, lr_model, lr_name, table, connection)\n",
    "        # df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        # ensemble_dataset = pd.DataFrame(columns = range(12))\n",
    "        # ensemble_dataset.columns = [\"rf_1\", \"rf_0\", \"rf_negative_1\", \"knn_1\", \"knn_0\", \"knn_negative_1\", \"mlp_1\", \"mlp_0\", \"mlp_negative_1\", \"lr_1\", \"lr_0\", \"lr_negative_1\"]\n",
    "        # ensemble_dataset[\"rf_1\"] = rf_pred_class1\n",
    "        # ensemble_dataset[\"rf_0\"] = rf_pred_class0\n",
    "        # ensemble_dataset[\"rf_negative_1\"] = rf_pred_class_negative_1\n",
    "        \n",
    "        # ensemble_dataset[\"knn_1\"] = knn_pred_class1\n",
    "        # ensemble_dataset[\"knn_0\"] = knn_pred_class0\n",
    "        # ensemble_dataset[\"knn_negative_1\"] = knn_pred_class_negative_1\n",
    "        \n",
    "        # ensemble_dataset[\"mlp_1\"] = mlp_pred_class1\n",
    "        # ensemble_dataset[\"mlp_0\"] = mlp_pred_class0\n",
    "        # ensemble_dataset[\"mlp_negative_1\"] = mlp_pred_class_negative_1\n",
    "        \n",
    "        # ensemble_dataset[\"lr_1\"] = lr_pred_class1\n",
    "        # ensemble_dataset[\"lr_0\"] = lr_pred_class0\n",
    "        # ensemble_dataset[\"lr_negative_1\"] = lr_pred_class_negative_1\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision = evaluation_ensemble(X_test, y_test, ensemble_model, ensemble_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "    return df_ml\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADA_1min_complete_1day_preprocessed_1day_features', 'BAT_1min_complete_1day_preprocessed_1day_features', 'BCH_1min_complete_1day_preprocessed_1day_features', 'BNT_1min_complete_1day_preprocessed_1day_features', 'BSV_1min_complete_1day_preprocessed_1day_features', 'BTC_1min_complete_1day_preprocessed_1day_features', 'BTG_1min_complete_1day_preprocessed_1day_features', 'DASH_1min_complete_1day_preprocessed_1day_features', 'DOGE_1min_complete_1day_preprocessed_1day_features', 'EOS_1min_complete_1day_preprocessed_1day_features', 'ETC_1min_complete_1day_preprocessed_1day_features', 'ETH_1min_complete_1day_preprocessed_1day_features', 'FUN_1min_complete_1day_preprocessed_1day_features', 'ICX_1min_complete_1day_preprocessed_1day_features', 'KNC_1min_complete_1day_preprocessed_1day_features', 'LINK_1min_complete_1day_preprocessed_1day_features', 'LRC_1min_complete_1day_preprocessed_1day_features', 'LSK_1min_complete_1day_preprocessed_1day_features', 'LTC_1min_complete_1day_preprocessed_1day_features', 'MKR_1min_complete_1day_preprocessed_1day_features', 'NEO_1min_complete_1day_preprocessed_1day_features', 'OMG_1min_complete_1day_preprocessed_1day_features', 'ONT_1min_complete_1day_preprocessed_1day_features', 'QTUM_1min_complete_1day_preprocessed_1day_features', 'REP_1min_complete_1day_preprocessed_1day_features', 'SC_1min_complete_1day_preprocessed_1day_features', 'SNT_1min_complete_1day_preprocessed_1day_features', 'TRX_1min_complete_1day_preprocessed_1day_features']\n",
      "training ensemble!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=2):\n",
      "ABNORMAL_TERMINATION_IN_LNSRCH.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "df_ml = apply_ml_algorithms_pooling(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>model</th>\n",
       "      <th>f1-score weighted</th>\n",
       "      <th>f1-score macro</th>\n",
       "      <th>recall macro</th>\n",
       "      <th>precision macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.247893</td>\n",
       "      <td>0.376531</td>\n",
       "      <td>0.302389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BAT_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.659106</td>\n",
       "      <td>0.368184</td>\n",
       "      <td>0.365869</td>\n",
       "      <td>0.384705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BCH_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.632770</td>\n",
       "      <td>0.316726</td>\n",
       "      <td>0.336429</td>\n",
       "      <td>0.355627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BNT_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.652184</td>\n",
       "      <td>0.368524</td>\n",
       "      <td>0.368258</td>\n",
       "      <td>0.397416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BSV_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.642905</td>\n",
       "      <td>0.362376</td>\n",
       "      <td>0.383796</td>\n",
       "      <td>0.362228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>BTC_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.616685</td>\n",
       "      <td>0.371658</td>\n",
       "      <td>0.371334</td>\n",
       "      <td>0.372062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BTG_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.689821</td>\n",
       "      <td>0.351762</td>\n",
       "      <td>0.368865</td>\n",
       "      <td>0.342454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DASH_1min_complete_1day_preprocessed_1day_feat...</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.672002</td>\n",
       "      <td>0.373118</td>\n",
       "      <td>0.386384</td>\n",
       "      <td>0.421681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>DOGE_1min_complete_1day_preprocessed_1day_feat...</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.666030</td>\n",
       "      <td>0.289406</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.255708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>EOS_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.545704</td>\n",
       "      <td>0.367037</td>\n",
       "      <td>0.401066</td>\n",
       "      <td>0.430664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ETC_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.608460</td>\n",
       "      <td>0.394764</td>\n",
       "      <td>0.412170</td>\n",
       "      <td>0.393648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ETH_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.599451</td>\n",
       "      <td>0.353138</td>\n",
       "      <td>0.352826</td>\n",
       "      <td>0.356707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>FUN_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.642234</td>\n",
       "      <td>0.344729</td>\n",
       "      <td>0.348154</td>\n",
       "      <td>0.346071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ICX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.635818</td>\n",
       "      <td>0.360011</td>\n",
       "      <td>0.357966</td>\n",
       "      <td>0.372235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>KNC_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.622464</td>\n",
       "      <td>0.338261</td>\n",
       "      <td>0.350284</td>\n",
       "      <td>0.328014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LINK_1min_complete_1day_preprocessed_1day_feat...</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.578245</td>\n",
       "      <td>0.352870</td>\n",
       "      <td>0.373138</td>\n",
       "      <td>0.474210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LRC_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.610912</td>\n",
       "      <td>0.308386</td>\n",
       "      <td>0.311399</td>\n",
       "      <td>0.319312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LSK_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.634836</td>\n",
       "      <td>0.294852</td>\n",
       "      <td>0.301023</td>\n",
       "      <td>0.290679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LTC_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.516402</td>\n",
       "      <td>0.356651</td>\n",
       "      <td>0.410211</td>\n",
       "      <td>0.399073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>MKR_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.613217</td>\n",
       "      <td>0.356241</td>\n",
       "      <td>0.360287</td>\n",
       "      <td>0.364945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NEO_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.668431</td>\n",
       "      <td>0.372487</td>\n",
       "      <td>0.395410</td>\n",
       "      <td>0.370743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>OMG_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.583893</td>\n",
       "      <td>0.348226</td>\n",
       "      <td>0.353440</td>\n",
       "      <td>0.376621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ONT_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.805116</td>\n",
       "      <td>0.701111</td>\n",
       "      <td>0.672753</td>\n",
       "      <td>0.762528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>QTUM_1min_complete_1day_preprocessed_1day_feat...</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.422850</td>\n",
       "      <td>0.238011</td>\n",
       "      <td>0.337557</td>\n",
       "      <td>0.298624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>REP_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.655893</td>\n",
       "      <td>0.287051</td>\n",
       "      <td>0.330935</td>\n",
       "      <td>0.253444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>SC_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.525645</td>\n",
       "      <td>0.328840</td>\n",
       "      <td>0.370819</td>\n",
       "      <td>0.351689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>SNT_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.570002</td>\n",
       "      <td>0.312160</td>\n",
       "      <td>0.338109</td>\n",
       "      <td>0.296409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.179931</td>\n",
       "      <td>0.133577</td>\n",
       "      <td>0.324447</td>\n",
       "      <td>0.293314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           table_name             model  \\\n",
       "0   ADA_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "1   BAT_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "2   BCH_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "3   BNT_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "4   BSV_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "5   BTC_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "6   BTG_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "7   DASH_1min_complete_1day_preprocessed_1day_feat...  ensemble_pooling   \n",
       "8   DOGE_1min_complete_1day_preprocessed_1day_feat...  ensemble_pooling   \n",
       "9   EOS_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "10  ETC_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "11  ETH_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "12  FUN_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "13  ICX_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "14  KNC_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "15  LINK_1min_complete_1day_preprocessed_1day_feat...  ensemble_pooling   \n",
       "16  LRC_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "17  LSK_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "18  LTC_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "19  MKR_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "20  NEO_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "21  OMG_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "22  ONT_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "23  QTUM_1min_complete_1day_preprocessed_1day_feat...  ensemble_pooling   \n",
       "24  REP_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "25   SC_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "26  SNT_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "27  TRX_1min_complete_1day_preprocessed_1day_features  ensemble_pooling   \n",
       "\n",
       "    f1-score weighted  f1-score macro  recall macro  precision macro  \n",
       "0            0.387700        0.247893      0.376531         0.302389  \n",
       "1            0.659106        0.368184      0.365869         0.384705  \n",
       "2            0.632770        0.316726      0.336429         0.355627  \n",
       "3            0.652184        0.368524      0.368258         0.397416  \n",
       "4            0.642905        0.362376      0.383796         0.362228  \n",
       "5            0.616685        0.371658      0.371334         0.372062  \n",
       "6            0.689821        0.351762      0.368865         0.342454  \n",
       "7            0.672002        0.373118      0.386384         0.421681  \n",
       "8            0.666030        0.289406      0.333333         0.255708  \n",
       "9            0.545704        0.367037      0.401066         0.430664  \n",
       "10           0.608460        0.394764      0.412170         0.393648  \n",
       "11           0.599451        0.353138      0.352826         0.356707  \n",
       "12           0.642234        0.344729      0.348154         0.346071  \n",
       "13           0.635818        0.360011      0.357966         0.372235  \n",
       "14           0.622464        0.338261      0.350284         0.328014  \n",
       "15           0.578245        0.352870      0.373138         0.474210  \n",
       "16           0.610912        0.308386      0.311399         0.319312  \n",
       "17           0.634836        0.294852      0.301023         0.290679  \n",
       "18           0.516402        0.356651      0.410211         0.399073  \n",
       "19           0.613217        0.356241      0.360287         0.364945  \n",
       "20           0.668431        0.372487      0.395410         0.370743  \n",
       "21           0.583893        0.348226      0.353440         0.376621  \n",
       "22           0.805116        0.701111      0.672753         0.762528  \n",
       "23           0.422850        0.238011      0.337557         0.298624  \n",
       "24           0.655893        0.287051      0.330935         0.253444  \n",
       "25           0.525645        0.328840      0.370819         0.351689  \n",
       "26           0.570002        0.312160      0.338109         0.296409  \n",
       "27           0.179931        0.133577      0.324447         0.293314  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_argmax_evaluation(model, X_train, table, db_connection):\n",
    "    \n",
    "    class_probabilities = model.predict_proba(X_train)\n",
    "    thresholds_long = []\n",
    "    for i in np.arange(0, 1, 0.02):\n",
    "        thresholds_long.append(i)\n",
    "        \n",
    "    i = 0\n",
    "    for threshold in thresholds_long:\n",
    "        class1 = class_probabilities[:, 0].copy()\n",
    "\n",
    "        class1[class1 > threshold] = 1\n",
    "        class1[class1 < threshold] = 0\n",
    "        \n",
    "        class1_str = [str(x) for x in class1.tolist()]\n",
    "        \n",
    "        df = pd.concat([X_train])\n",
    "        df[\"buy_short_indicator\"] = class1_str\n",
    "        df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "        df.to_sql(f\"no_{i}_threshold_ensemble_long_{table}\", db_connection, if_exists=\"replace\")\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    thresholds_short = []\n",
    "    for i in np.arange(0, 1, 0.02):\n",
    "        thresholds_short.append(i)\n",
    "    \n",
    "    k = 0\n",
    "    for threshold_short in thresholds_short:\n",
    "        print(k)\n",
    "        class3 = class_probabilities[:, 2].copy()\n",
    "\n",
    "        class3[class3 < threshold_short] = 0\n",
    "        class3[class3 > threshold_short] = -1\n",
    "        class3_str = [str(x) for x in class3.tolist()]\n",
    "        \n",
    "        df = pd.concat([X_train])\n",
    "        df[\"buy_short_indicator\"] = class3_str\n",
    "        df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "        df.to_sql(f\"no_{k}_threshold_ensemble_short_{table}\", db_connection, if_exists=\"replace\")\n",
    "        \n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_alternative_argmax_evaluation(db_connection):\n",
    "\n",
    "    table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\", db_connection)\n",
    "    \n",
    "    table_names_list = table_names['name'].tolist()\n",
    "\n",
    "    filtered_table_names = [name for name in table_names_list if \"_1day_features\" in name and 'trades' not in name and 'equity_curve' not in name and '_pooling' not in name and \"ensemble\" not in name]\n",
    "    print(filtered_table_names)\n",
    "    for table in filtered_table_names:\n",
    "        df_temp = pd.read_sql_query(f\"select * from {table}\", connection)\n",
    "        \n",
    "        y = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "        y = y.fillna(0)\n",
    "        y = y.astype(str)\n",
    "        \n",
    "        X = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\", \"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1) \n",
    "        X_train = X.iloc[:-365]\n",
    "\n",
    "        X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "\n",
    "        \n",
    "        alternative_argmax_evaluation(rf_new, X_train, table, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_alternative_argmax_evaluation(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_argmax_evaluation(model, X_test, y_test, table, db_connection):\n",
    "    \n",
    "    class_probabilities = model.predict_proba(X_test)\n",
    "    \n",
    "    class1 = class_probabilities[:, 0].copy()\n",
    "    class3 = class_probabilities[:, 2].copy()\n",
    "    \n",
    "    class1[class1 >= 0.34] = 1\n",
    "    class1[class1 < 0.34] = 0\n",
    "\n",
    "    class3[class3 < 0.42] = 0\n",
    "    class3[class3 >= 0.42] = -1\n",
    "\n",
    "    \n",
    "    y_pred = class3 + class1\n",
    "    \n",
    "    y_pred1 = y_pred.tolist()\n",
    "    y_pred = [str(x) for x in y_pred1]\n",
    "    \n",
    "    f1score = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1score_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    df = pd.concat([X_test])\n",
    "    df[\"buy_short_indicator\"] = y_pred\n",
    "    df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "    \n",
    "    df.to_sql(f\"{table}_ensemble_pooling_final\", db_connection, if_exists=\"replace\")\n",
    "    \n",
    "    return f1score, f1score_macro, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_alternative_argmax_evaluation(db_connection):\n",
    "\n",
    "    table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\", db_connection)\n",
    "    \n",
    "    table_names_list = table_names['name'].tolist()\n",
    "\n",
    "    filtered_table_names = [name for name in table_names_list if \"_1day_features\" in name and 'trades' not in name and 'equity_curve' not in name and '_pooling' not in name and \"ensemble\" not in name]\n",
    "    print(filtered_table_names)\n",
    "    for table in filtered_table_names:\n",
    "        df_temp = pd.read_sql_query(f\"select * from {table}\", connection)\n",
    "        \n",
    "        y = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "        y = y.fillna(0)\n",
    "        y = y.astype(str)\n",
    "        \n",
    "        X = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\",\"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1)\n",
    "        \n",
    "        \n",
    "        X_test = X.iloc[-365:]\n",
    "        X_test[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_test[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "        y_test = y.iloc[-365:]\n",
    "\n",
    "        \n",
    "        f1score, f1score_macro, recall, precision = best_argmax_evaluation(rf_new, X_test, y_test, table, db_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "execute_alternative_argmax_evaluation(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cf7c0b8ed4c815043e48994c1e64c08f9d96fdc49c73ce762547f36d7ce0a11b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
