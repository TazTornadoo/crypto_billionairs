{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_storage import create_connection\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pickle\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = create_connection(\"../database/crypto_billionairs.db\")\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(X_train, y_train):\n",
    "    \n",
    "    clf = RandomForestClassifier(criterion=\"entropy\", min_samples_split= 0.01, min_samples_leaf= 0.005, max_depth=10, class_weight=\"balanced_subsample\")\n",
    "    print(\"training random forest!\")\n",
    "    clf.fit(X_train, y_train)\n",
    "   \n",
    "    return \"random_forest\", clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(X_train, y_train):\n",
    "    \n",
    "    neigh = KNeighborsClassifier(weights=\"uniform\", n_neighbors=5, algorithm=\"ball_tree\")\n",
    "    print(\"training knn!\")\n",
    "    neigh.fit(X_train, y_train)\n",
    "    \n",
    "    return \"knn\", neigh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_machine(X_train, y_train):\n",
    "    \n",
    "    svc = SVC(kernel=\"poly\", degree=4, C=1)\n",
    "    print(\"training svc!\")\n",
    "    svc.fit(X_train, y_train)\n",
    "    \n",
    "    return \"support_vector_classifier\", svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(X_train, y_train):\n",
    "    \n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"tanh\", solver=\"lbfgs\", learning_rate=\"constant\", learning_rate_init=2e-5, tol=1e-5)\n",
    "    print(\"training mlp!\")\n",
    "    mlp.fit(X_train, y_train)\n",
    "    \n",
    "    return \"mlp_classifier\", mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X_train, y_train):\n",
    "    global lg\n",
    "    lg = LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=1)\n",
    "    print(\"training lr!\")\n",
    "    lg.fit(X_train, y_train)\n",
    "    \n",
    "    return \"logistic_regression\", lg\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ensemble(rf_model, knn_model, mlp_model, lr_model, X_train, y_train):\n",
    "    \n",
    "    print(\"training ensemble!\")\n",
    "    \n",
    "    level0 = list()\n",
    "    level0.append(('lg', LogisticRegression(solver=\"liblinear\", penalty=\"l1\", C=1)))\n",
    "    level0.append(('knn', KNeighborsClassifier(weights=\"uniform\", n_neighbors=5, algorithm=\"ball_tree\")))\n",
    "    level0.append(('rf', RandomForestClassifier(criterion=\"entropy\", min_samples_split= 0.01, min_samples_leaf= 0.005, max_depth=10, class_weight=\"balanced_subsample\")))\n",
    "    level0.append(('mlp', MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"tanh\", solver=\"lbfgs\", learning_rate=\"constant\", learning_rate_init=2e-5, tol=1e-5)))\n",
    "    #mlp = MLPClassifier(hidden_layer_sizes=(10, 10), activation=\"tanh\", solver=\"lbfgs\", learning_rate=\"constant\", learning_rate_init=2e-5, tol=1e-5)\n",
    "    #mlp.fit(ensemble_dataset, y_train)\n",
    "    \n",
    "    level1 = RandomForestClassifier(criterion=\"entropy\")\n",
    "    rf_new = StackingClassifier(estimators=level0, final_estimator=level1)\n",
    "    rf_new.fit(X_train, y_train)\n",
    "    \n",
    "    return \"ensemble\", rf_new\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(X_test, y_test, model, model_name, table, db_connection):\n",
    "    \n",
    "    X_test_normalized = X_test.copy()\n",
    "    X_test_normalized[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_test_normalized[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "    y_pred = model.predict(X_test_normalized)\n",
    "    y_pred_proba = model.predict_proba(X_test_normalized)\n",
    "    \n",
    "    f1score = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1score_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    y_pred = y_pred.tolist()\n",
    "    df = pd.concat([X_test])\n",
    "    df[\"buy_short_indicator\"] = y_pred\n",
    "    df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "    \n",
    "    df.to_sql(f\"{table}_{model_name}_pooling\", db_connection, if_exists=\"replace\")\n",
    "    \n",
    "    return f\"{model_name}_pooling\", f1score, f1score_macro, recall, precision, y_pred_proba[:, 0], y_pred_proba[:, 1], y_pred_proba[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_ensemble(market_data, X_test, y_test, model, model_name, table, db_connection):\n",
    "    \n",
    "    y_pred = model.predict(market_data)\n",
    "    f1score = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    f1score_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    y_pred = y_pred.tolist()\n",
    "    df = pd.concat([market_data])\n",
    "    df[\"buy_short_indicator\"] = y_pred\n",
    "    df['close_buy_short_indicator'] = df[\"buy_short_indicator\"].shift(1).fillna(0.0)\n",
    "    \n",
    "    df.to_sql(f\"{table}_{model_name}_pooling\", db_connection, if_exists=\"replace\")\n",
    "    \n",
    "    return f\"{model_name}_pooling\", f1score, f1score_macro, recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ml_algorithms_pooling(db_connection):\n",
    "    \n",
    "    df_temp = pd.read_sql_query(f\"select * from cryptocurrency_pooling_dataset\", db_connection)\n",
    "    df_temp = shuffle(df_temp, random_state=42069)\n",
    "    \n",
    "    y_train = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "    y_train = y_train.fillna(0)\n",
    "    y_train = y_train.astype(str)\n",
    "        \n",
    "    X_train = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\",\"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1)\n",
    "    \n",
    "    X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]] = scaler.fit_transform(X_train[[\"open\", \"close\", \"high\", \"low\", \"volume\"]])\n",
    "    \n",
    "    ros = RandomOverSampler(random_state=0)\n",
    "    X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "    \n",
    "    table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\", db_connection)\n",
    "    table_names_list = table_names['name'].tolist()\n",
    "    filtered_table_names = [name for name in table_names_list if \"_1day_features\" in name and 'trades' not in name and 'equity_curve' not in name and '_pooling' not in name and \"_threshold_ensemble\" not in name]\n",
    "    print(filtered_table_names)\n",
    "    \n",
    "    rf_name, rf_model = random_forest(X_resampled, y_resampled)\n",
    "    knn_name, knn_model = knn(X_resampled, y_resampled)\n",
    "    #svc_name, svc_model = support_vector_machine(X_resampled, y_resampled)\n",
    "    mlp_name, mlp_model = mlp(X_resampled, y_resampled)\n",
    "    lr_name, lr_model = logistic_regression(X_resampled, y_resampled)\n",
    "    \n",
    "    ensemble_name, ensemble_model = model_ensemble(rf_model, knn_model, mlp_model, lr_model, X_train, y_train)\n",
    "    #creating the evaluation metric\n",
    "    df_ml = pd.DataFrame(columns = range(6))\n",
    "    df_ml.columns = [\"table_name\", \"model\", \"f1-score weighted\", \"f1-score macro\", \"recall macro\", \"precision macro\"]\n",
    "    \n",
    "    \n",
    "    for table in filtered_table_names:\n",
    "        \n",
    "        df_temp = pd.read_sql_query(f\"select * from {table}\", db_connection)\n",
    "        y = df_temp[\"buy_indicator\"] + df_temp[\"short_indicator\"]\n",
    "        y = y.fillna(0)\n",
    "        y = y.astype(str)\n",
    "        X = df_temp.drop([\"return\", \"buy_indicator\", \"short_indicator\",\"close_buy_indicator\", \"close_short_indicator\", \"time\", \"index\", \"level_0\", \"market_cap\"], axis=1)\n",
    "\n",
    "        X_test = X.iloc[-365:]\n",
    "        y_test = y.iloc[-365:]\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision, rf_pred_class1, rf_pred_class0, rf_pred_class_negative_1 = evaluation(X_test, y_test, rf_model, rf_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision, knn_pred_class1, knn_pred_class0, knn_pred_class_negative_1 = evaluation(X_test, y_test, knn_model, knn_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        # string, score, f1score_macro, recall, precision = evaluation(X_test, y_test, svc_model, svc_name, table, connection)\n",
    "        # df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision, mlp_pred_class1, mlp_pred_class0, mlp_pred_class_negative_1 = evaluation(X_test, y_test, mlp_model, mlp_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision, lr_pred_class1, lr_pred_class0, lr_pred_class_negative_1 = evaluation(X_test, y_test, lr_model, lr_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "        ensemble_dataset = pd.DataFrame(columns = range(12))\n",
    "        ensemble_dataset.columns = [\"rf_1\", \"rf_0\", \"rf_negative_1\", \"knn_1\", \"knn_0\", \"knn_negative_1\", \"mlp_1\", \"mlp_0\", \"mlp_negative_1\", \"lr_1\", \"lr_0\", \"lr_negative_1\"]\n",
    "        ensemble_dataset[\"rf_1\"] = rf_pred_class1\n",
    "        ensemble_dataset[\"rf_0\"] = rf_pred_class0\n",
    "        ensemble_dataset[\"rf_negative_1\"] = rf_pred_class_negative_1\n",
    "        \n",
    "        ensemble_dataset[\"knn_1\"] = knn_pred_class1\n",
    "        ensemble_dataset[\"knn_0\"] = knn_pred_class0\n",
    "        ensemble_dataset[\"knn_negative_1\"] = knn_pred_class_negative_1\n",
    "        \n",
    "        ensemble_dataset[\"mlp_1\"] = mlp_pred_class1\n",
    "        ensemble_dataset[\"mlp_0\"] = mlp_pred_class0\n",
    "        ensemble_dataset[\"mlp_negative_1\"] = mlp_pred_class_negative_1\n",
    "        \n",
    "        ensemble_dataset[\"lr_1\"] = lr_pred_class1\n",
    "        ensemble_dataset[\"lr_0\"] = lr_pred_class0\n",
    "        ensemble_dataset[\"lr_negative_1\"] = lr_pred_class_negative_1\n",
    "        \n",
    "        string, score, f1score_macro, recall, precision = evaluation_ensemble(X_test, ensemble_dataset, y_test, ensemble_model, ensemble_name, table, connection)\n",
    "        df_ml.loc[len(df_ml)] = [table, string, score, f1score_macro, recall, precision]\n",
    "        \n",
    "    return df_ml\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADA_1min_complete_1day_preprocessed_1day_features', 'BAT_1min_complete_1day_preprocessed_1day_features', 'BCH_1min_complete_1day_preprocessed_1day_features', 'BNT_1min_complete_1day_preprocessed_1day_features', 'BSV_1min_complete_1day_preprocessed_1day_features', 'BTC_1min_complete_1day_preprocessed_1day_features', 'BTG_1min_complete_1day_preprocessed_1day_features', 'DASH_1min_complete_1day_preprocessed_1day_features', 'DOGE_1min_complete_1day_preprocessed_1day_features', 'EOS_1min_complete_1day_preprocessed_1day_features', 'ETC_1min_complete_1day_preprocessed_1day_features', 'ETH_1min_complete_1day_preprocessed_1day_features', 'FUN_1min_complete_1day_preprocessed_1day_features', 'ICX_1min_complete_1day_preprocessed_1day_features', 'KNC_1min_complete_1day_preprocessed_1day_features', 'LINK_1min_complete_1day_preprocessed_1day_features', 'LRC_1min_complete_1day_preprocessed_1day_features', 'LSK_1min_complete_1day_preprocessed_1day_features', 'LTC_1min_complete_1day_preprocessed_1day_features', 'MKR_1min_complete_1day_preprocessed_1day_features', 'NEO_1min_complete_1day_preprocessed_1day_features', 'OMG_1min_complete_1day_preprocessed_1day_features', 'ONT_1min_complete_1day_preprocessed_1day_features', 'QTUM_1min_complete_1day_preprocessed_1day_features', 'REP_1min_complete_1day_preprocessed_1day_features', 'SC_1min_complete_1day_preprocessed_1day_features', 'SNT_1min_complete_1day_preprocessed_1day_features', 'TRX_1min_complete_1day_preprocessed_1day_features']\n",
      "training random forest!\n",
      "training knn!\n",
      "training mlp!\n",
      "training lr!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training ensemble!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\Users\\janfa\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:559: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "df_ml = apply_ml_algorithms_pooling(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>table_name</th>\n",
       "      <th>model</th>\n",
       "      <th>f1-score weighted</th>\n",
       "      <th>f1-score macro</th>\n",
       "      <th>recall macro</th>\n",
       "      <th>precision macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>random_forest_pooling</td>\n",
       "      <td>0.628604</td>\n",
       "      <td>0.441049</td>\n",
       "      <td>0.452483</td>\n",
       "      <td>0.437672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>knn_pooling</td>\n",
       "      <td>0.077527</td>\n",
       "      <td>0.099956</td>\n",
       "      <td>0.344697</td>\n",
       "      <td>0.316983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>mlp_classifier_pooling</td>\n",
       "      <td>0.029379</td>\n",
       "      <td>0.076052</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.042922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>logistic_regression_pooling</td>\n",
       "      <td>0.533613</td>\n",
       "      <td>0.319573</td>\n",
       "      <td>0.385943</td>\n",
       "      <td>0.322812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ADA_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.168054</td>\n",
       "      <td>0.144276</td>\n",
       "      <td>0.355765</td>\n",
       "      <td>0.271390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>random_forest_pooling</td>\n",
       "      <td>0.604595</td>\n",
       "      <td>0.385964</td>\n",
       "      <td>0.422643</td>\n",
       "      <td>0.392267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>knn_pooling</td>\n",
       "      <td>0.046267</td>\n",
       "      <td>0.096394</td>\n",
       "      <td>0.314266</td>\n",
       "      <td>0.212135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>mlp_classifier_pooling</td>\n",
       "      <td>0.619007</td>\n",
       "      <td>0.352631</td>\n",
       "      <td>0.385583</td>\n",
       "      <td>0.368579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>logistic_regression_pooling</td>\n",
       "      <td>0.573666</td>\n",
       "      <td>0.300381</td>\n",
       "      <td>0.357740</td>\n",
       "      <td>0.308624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>TRX_1min_complete_1day_preprocessed_1day_features</td>\n",
       "      <td>ensemble_pooling</td>\n",
       "      <td>0.077926</td>\n",
       "      <td>0.087202</td>\n",
       "      <td>0.298625</td>\n",
       "      <td>0.210106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            table_name  \\\n",
       "0    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "1    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "2    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "3    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "4    ADA_1min_complete_1day_preprocessed_1day_features   \n",
       "..                                                 ...   \n",
       "135  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "136  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "137  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "138  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "139  TRX_1min_complete_1day_preprocessed_1day_features   \n",
       "\n",
       "                           model  f1-score weighted  f1-score macro  \\\n",
       "0          random_forest_pooling           0.628604        0.441049   \n",
       "1                    knn_pooling           0.077527        0.099956   \n",
       "2         mlp_classifier_pooling           0.029379        0.076052   \n",
       "3    logistic_regression_pooling           0.533613        0.319573   \n",
       "4               ensemble_pooling           0.168054        0.144276   \n",
       "..                           ...                ...             ...   \n",
       "135        random_forest_pooling           0.604595        0.385964   \n",
       "136                  knn_pooling           0.046267        0.096394   \n",
       "137       mlp_classifier_pooling           0.619007        0.352631   \n",
       "138  logistic_regression_pooling           0.573666        0.300381   \n",
       "139             ensemble_pooling           0.077926        0.087202   \n",
       "\n",
       "     recall macro  precision macro  \n",
       "0        0.452483         0.437672  \n",
       "1        0.344697         0.316983  \n",
       "2        0.333333         0.042922  \n",
       "3        0.385943         0.322812  \n",
       "4        0.355765         0.271390  \n",
       "..            ...              ...  \n",
       "135      0.422643         0.392267  \n",
       "136      0.314266         0.212135  \n",
       "137      0.385583         0.368579  \n",
       "138      0.357740         0.308624  \n",
       "139      0.298625         0.210106  \n",
       "\n",
       "[140 rows x 6 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alternative_argmax_evaluation(model, X_train, y_train, db_connection):\n",
    "    \n",
    "    class_probabilities = model.predict_proba(X_train)\n",
    "    \n",
    "    class1 = class_probabilities[:, 0]\n",
    "    class0 = class_probabilities[:, 1]\n",
    "    class_negative1 = class_probabilities[:, 2]\n",
    "    \n",
    "    print(class1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_alternative_argmax_evaluation(db_connection)\n",
    "\n",
    "    table_names = pd.read_sql_query(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name;\", db_connection)\n",
    "    \n",
    "    table_names_list = table_names['name'].tolist()\n",
    "\n",
    "    filtered_table_names = [name for name in table_names_list if \"_1day_features\" in name and 'trades' not in name and 'equity_curve' not in name and '_pooling' not in name]\n",
    "    print(filtered_table_names)\n",
    "    for table in filtered_table_names:"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cf7c0b8ed4c815043e48994c1e64c08f9d96fdc49c73ce762547f36d7ce0a11b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
